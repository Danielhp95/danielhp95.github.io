<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Reinforce my Learning</title>
    <description></description>
    <link>http://localhost:4000/ReinforceMyLearning//</link>
    <atom:link href="http://localhost:4000/ReinforceMyLearning//feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 22 Jul 2018 22:36:11 +0100</pubDate>
    <lastBuildDate>Sun, 22 Jul 2018 22:36:11 +0100</lastBuildDate>
    <generator>Jekyll v3.6.2</generator>
    
      <item>
        <title>Reinforcement Learning a technical introduction</title>
        <description>&lt;h1 id=&quot;what-is-reinforcement-learning&quot;&gt;What is Reinforcement Learning?&lt;/h1&gt;

&lt;h2 id=&quot;short-answer&quot;&gt;Short answer&lt;/h2&gt;
&lt;p&gt;Reinforcement learning (RL) is an optimization framework.&lt;/p&gt;

&lt;h2 id=&quot;long-answer&quot;&gt;Long answer&lt;/h2&gt;

&lt;p&gt;A problem can be considered a reinforcement learning problem if it can be framed in the following way: Given an enviroment in which an agent can take actions, receiving a reward for each action, find a policy that maximizes the expected cumulative reward that the agent will obtain by acting in the environment.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/ReinforceMyLearning//assets/images/posts/rl-loop.png&quot; alt=&quot;reinforcement-learning-loop&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;markov-decision-processes&quot;&gt;Markov Decision Processes&lt;/h3&gt;

&lt;p&gt;The most famous mathematical structure used to represent reinforcement learning environments are Markov Decision Processes (MDP) (Bellman 1957). Bellman introduced the concept of a Markov Decision Process as an extension of the famous mathematical construct of Markov chains. Markov Decision Processes are a standard model for sequential decision making and control problems. An MDP is fully defined by the 5-tuple &lt;script type=&quot;math/tex&quot;&gt;(\mathcal{S}, \mathcal{A}, \mathcal{P(\cdot \vert \cdot, \cdot)}, \mathcal{R(\cdot, \cdot)}, \gamma)&lt;/script&gt;. Whereby:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathcal{S}&lt;/script&gt; is the set of states &lt;script type=&quot;math/tex&quot;&gt;s \in \mathcal{S}&lt;/script&gt; of the underlying Markov chain, where &lt;script type=&quot;math/tex&quot;&gt;s_t \in \mathcal{S}&lt;/script&gt; represents the state of the environment at time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathcal{A}&lt;/script&gt; is the set of actions &lt;script type=&quot;math/tex&quot;&gt;a \in \mathcal{A}&lt;/script&gt; which are the transition labels between states of the underlying Markov chain. &lt;script type=&quot;math/tex&quot;&gt;A_t \subset \mathcal{A}&lt;/script&gt; is the subset of available actions in state &lt;script type=&quot;math/tex&quot;&gt;s_t&lt;/script&gt; at time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;. If an state &lt;script type=&quot;math/tex&quot;&gt;s_t&lt;/script&gt; has no available actions, it is said to be a \textit{terminal} state.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathcal{P}(s_{t+1} \vert s_t, a_t) \in [0, 1]&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;s_t, s_{t+1} \in \mathcal{S}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;a_t \in \mathcal{A}&lt;/script&gt;. &lt;script type=&quot;math/tex&quot;&gt;\mathcal{P}&lt;/script&gt; is the transition probability function (The function &lt;script type=&quot;math/tex&quot;&gt;\mathcal{P}&lt;/script&gt; is also known in the literature as the transition probability kernel, or the transition kernel~\citep{Tamar2017}. The word kernel is a heavily overloaded mathematical term that refers to a function that maps a series of inputs to value in &lt;script type=&quot;math/tex&quot;&gt;\mathbb{R}&lt;/script&gt;). It defines the probability of transitioning to state &lt;script type=&quot;math/tex&quot;&gt;s_{t+1}&lt;/script&gt; from state &lt;script type=&quot;math/tex&quot;&gt;s_t&lt;/script&gt; after performig action &lt;script type=&quot;math/tex&quot;&gt;a_t&lt;/script&gt;. Thus, &lt;script type=&quot;math/tex&quot;&gt;\mathcal{P}: \mathcal{S} \times \mathcal{A} \to [0,1]&lt;/script&gt;. Given a state &lt;script type=&quot;math/tex&quot;&gt;s_t&lt;/script&gt; and an action &lt;script type=&quot;math/tex&quot;&gt;a_t&lt;/script&gt; at time &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;, we can find the next state &lt;script type=&quot;math/tex&quot;&gt;s_{t+1}&lt;/script&gt; by sampling from the distribution &lt;script type=&quot;math/tex&quot;&gt;s_{t+1} \sim P(s_t, a_t)&lt;/script&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\mathcal{R}(s_t, a_t, s_{t+1}) \in \mathbb{R}&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;s_t, s_{t+1} \in \mathcal{S}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;a_t \in \mathcal{A}&lt;/script&gt;. &lt;script type=&quot;math/tex&quot;&gt;\mathcal{R}&lt;/script&gt; is the reward function, which returns the immediate reward of performing action &lt;script type=&quot;math/tex&quot;&gt;a_t&lt;/script&gt; in state &lt;script type=&quot;math/tex&quot;&gt;s_t&lt;/script&gt; and ending in state &lt;script type=&quot;math/tex&quot;&gt;s_{t+1}&lt;/script&gt;. The real-valued reward (The reward &lt;script type=&quot;math/tex&quot;&gt;r_t&lt;/script&gt; can be equivalently written as &lt;script type=&quot;math/tex&quot;&gt;r(s_t, a_t)&lt;/script&gt;) &lt;script type=&quot;math/tex&quot;&gt;r_t&lt;/script&gt; is typically in the range &lt;script type=&quot;math/tex&quot;&gt;[-1,-1]&lt;/script&gt;. &lt;script type=&quot;math/tex&quot;&gt;\mathcal{R}: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \to \mathbb{R}&lt;/script&gt;. If the environment is deterministic, the reward function can be rewritten as &lt;script type=&quot;math/tex&quot;&gt;\mathcal{R}(s_t, a_t)&lt;/script&gt; because the state transition defined by &lt;script type=&quot;math/tex&quot;&gt;\mathcal{P}(s_t, a_t)&lt;/script&gt; is deterministic.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\gamma \in [0,1]&lt;/script&gt; is the discount factor, which represent the rate of importance between immediate and future rewards. If &lt;script type=&quot;math/tex&quot;&gt;\gamma = 0&lt;/script&gt; the agent cares only about the immediate reward, if &lt;script type=&quot;math/tex&quot;&gt;\gamma = 1&lt;/script&gt; all rewards &lt;script type=&quot;math/tex&quot;&gt;r_t&lt;/script&gt; are taken into account. &lt;script type=&quot;math/tex&quot;&gt;\gamma&lt;/script&gt; is often used as a variance reduction method, and aids proofs in infinitely running environments (Sutton 1999).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The environment is sometimes represented by the Greek letter &lt;script type=&quot;math/tex&quot;&gt;\xi&lt;/script&gt;. The tuple of elementes introduced above are the core components of any environment &lt;script type=&quot;math/tex&quot;&gt;\xi&lt;/script&gt;. Finally, a lot of work in RL literature also presents a distribution over initial states &lt;script type=&quot;math/tex&quot;&gt;\rho_0&lt;/script&gt; of the MDP, So that the initial state can be sampled from it: &lt;script type=&quot;math/tex&quot;&gt;s_0 \sim \rho_0&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;An environment can be episodic if it presents terminal states, or if there are a fixed number of steps after which the environment will not accept any more actions. However environments can also run infinitely.&lt;/p&gt;

&lt;p&gt;Acting inside of the environment, there is the agent, and through its actions the transitions between the MDP states are triggered, advancing the environment state and obtaining rewards. The agentâ€™s behaviour is fully defined by its policy &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt;. A policy &lt;script type=&quot;math/tex&quot;&gt;\pi(a_t \vert s_t) \in [0,1]&lt;/script&gt;, where &lt;script type=&quot;math/tex&quot;&gt;s_t \in \mathcal{S}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;a_t \in \mathcal{A}&lt;/script&gt; is a mapping from states to a distribution over actions. Given a state &lt;script type=&quot;math/tex&quot;&gt;s_t&lt;/script&gt; it is possible to sample an action &lt;script type=&quot;math/tex&quot;&gt;a_t&lt;/script&gt; from the policy distribution &lt;script type=&quot;math/tex&quot;&gt;a_t \sim \pi(s_t)&lt;/script&gt;. Thus, &lt;script type=&quot;math/tex&quot;&gt;\pi: \mathcal{S} \times \mathcal{A} \to [0,1]&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;The reinforcement learning loop presented in image above can be represented in algorithmic form as follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Sample initial state from the initial state distribution &lt;script type=&quot;math/tex&quot;&gt;s_0 \sim \rho_0&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;t \leftarrow 0&lt;/script&gt;.&lt;br /&gt;
Repeat until &lt;script type=&quot;math/tex&quot;&gt;Termination&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\;\;\;&lt;/script&gt; Sample action &lt;script type=&quot;math/tex&quot;&gt;a_t \sim \pi(s_t)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\;\;\;&lt;/script&gt; Sample successor state from the transition probability function &lt;script type=&quot;math/tex&quot;&gt;s_{t+1} \sim P(s_t, a_t)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\;\;\;&lt;/script&gt; Sample reward from reward function &lt;script type=&quot;math/tex&quot;&gt;r_t \sim R(s_t, a_t, s_{t+1})&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\;\;\;&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;t \leftarrow t + 1&lt;/script&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For an episode of length &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt;, The objective for the agent is to find an \textit{optimal} policy &lt;script type=&quot;math/tex&quot;&gt;\pi^*&lt;/script&gt;, which maximizes the cumulative sum of (possibly discounted) rewards.&lt;br /&gt;
&lt;img src=&quot;/ReinforceMyLearning//assets/images/posts/optimal-policy-equation.png&quot; alt=&quot;optimal-policy-equation&quot; height=&quot;120&quot; width=&quot;700&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;references&quot;&gt;References&lt;/h4&gt;

&lt;p&gt;Bellman 1957&lt;br /&gt;
Sutton 1999&lt;/p&gt;
</description>
        <pubDate>Fri, 21 Jul 2017 05:00:00 +0100</pubDate>
        <link>http://localhost:4000/ReinforceMyLearning//technical-introduction-to-reinforcement-learning</link>
        <guid isPermaLink="true">http://localhost:4000/ReinforceMyLearning//technical-introduction-to-reinforcement-learning</guid>
        
        <category>Reinforcement</category>
        
        <category>Learning,</category>
        
        <category>Markov</category>
        
        <category>Decision</category>
        
        <category>Process</category>
        
        
      </item>
    
  </channel>
</rss>
